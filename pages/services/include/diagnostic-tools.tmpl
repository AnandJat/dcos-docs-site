DC/OS clusters provide several tools for diagnosing problems with a running {{ model.techName }} service. The service itself also has endpoints for diagnosing issues.

## Plans

The first step to diagnosing any service issue is to view the running services plans. A list of plans can be retrieved by running:
```bash
dcos {{ model.packageName }} --name=<service-name> plan list
```

If deployment or an update is not proceeding as expected, check the status of the `deploy` plan by running:
```bash
dcos {{ model.packageName }} --name=<service-name> plan show deploy
```

If a failed task is not recovering as expected, check the status of the `recovery` plan by running:
```bash
dcos {{ model.packageName }} --name=<service-name> plan show recovery
```

The current state of the relevant plan can provide you guidance on where to look to root cause an issue. A rough guide is:
- Can't talk to service scheduler (i.e., `plan list` fails), investigate the service scheduler's status in Marathon.
- The deploy plan is stuck and not making progress
	- A step is continually failing, investigate task and service scheduler logs to find the root cause.
	- The plan is not proceeding and not launching tasks, investigate the scheduler logs to find the root cause.
- The recovery plan is stuck and not making progress
	- A step is continually failing, investigate task and service scheduler logs to find the root cause.
	- The plan is not proceeding and not launching tasks, investigate the scheduler logs to find the root cause.



## Logging

DC/OS has three ways to access service scheduler and service task logs.
1. Via the DC/OS GUI
1. Via the Mesos GUI
1. Via the DC/OS CLI

### DC/OS GUI

A service's logs are accessed by selecting the service in the **Services** tab. Both the service scheduler and service tasks are displayed side by side in this view.

Clicking on a task presents you with the option to view the tasks sandbox (files) as well as its stdout and stderr.

### Mesos GUI

The Mesos GUI provides similar access as that of the DC/OS UI, but the service scheduler is separate from the service tasks. The service tasks can all be found in the **Frameworks** tab under the framework with the same name as the service. The service scheduler can be found in the Marathon framework, it will be a task with the same name as the service.

Access both the files and logs of a service by clicking on its `sandbox` link.

[<img src="/services/include/img/ops-guide-scheduler-sandbox.png" alt="contents of a scheduler sandbox" width="400"/>](/services/include/img/ops-guide-scheduler-sandbox.png)

### DC/OS CLI

The `dcos task log` subcommand allows you to pull logs from multiple tasks at the same time. The `dcos task log <task-pattern>` command will fetch logs for all tasks matching the prefix pattern. See the help of the subcommand for full details of the command.

### Mesos Agent logs

Occasionally, it can also be useful to examine what a given Mesos agent is doing. The Mesos Agent handles deployment of Mesos tasks to a given physical system in the cluster. One Mesos Agent runs on each system. These logs can be useful for determining if there's a problem at the system level that is causing alerts across multiple services on that system.

Mesos agent logs can be accessed via the Mesos GUI or directly on the agent. The GUI method is described here.

Navigate to the agent you want to view either directly from a task by clicking the "Agent" item in the breadcrumb when viewing a task (this will go directly to the agent hosting the task), or by navigating through the "Agents" menu item at the top of the screen (you will need to select the desired agent from the list).

In the Agent view, you'll see a list of frameworks with a presence on that Agent. In the left pane you'll see a plain link named "LOG". Click that link to view the agent logs.

[<img src="/services/include/img/ops-guide-agent.png" alt="view of tasks running on a given agent" width="400"/>](/services/include/img/ops-guide-agent.png)

## Managing Service Pods

## Fixing Broken Tasks

Some situations may require manually intervening in a task to allow it to recover.

#### Prerequisites

- A [recent version of the DC/OS CLI](https://dcos.io/docs/latest/usage/cli/install/) with support for the `task exec` command.
- A version of the DC/OS {{ model.techName }} service that supports pausing a pod.
- The {{ model.packageName }} CLI installed.

### `dcos task exec`

The `dcos task exec` command allows you to enter the container of a running task and execute commands within the task's context.
```bash
$ dcos task exec <task-pattern> <flags> <command>
```

Note: `<task-pattern>` must match exactly one task.

To start an interactive session, use the `-it` flags (short for `--interactive --tty`).
```bash
$ dcos task exec -it <task-pattern> /bin/bash
```

<strong>Note:</strong> Any changes made to a task (except those made to a persistent volume) will NOT persist across task restarts. Do not use `task exec` to modify task configuration.


### Pod Pause

`dcos task exec` is a useful tool for modifying tasks, but a crash looping task is very difficult to modify. That's where pod pause comes in. It offers the ability to pause the tasks in a pod.

First, identify the pod to pause by listing all service pods.
```bash
$ dcos {{ model.packageName }} pod list
```

After selecting a pod to pause, execute `pod pause` against it.
```bash
$ dcos {{ model.packageName }} debug pod pause <pod-name>
```

This will relaunch the tasks of the pod with an overwritten command of an infinite sleep. The tasks will however have all the URIs and environment as though they were running. After making any necessary modifications to the task(s), resume the pod.
```bash
$ dcos {{ model.packageName }} debug pod resume <pod-name>
```

Use the `pod status` command to keep track of what pods may be in an altered state or to confirm a pod has been paused.
```bash
$ dcos {{ model.packageName }} pod status
```

Note: It is possible to pause an individual task of a pod by passing the `-t` flag to the `pod pause` command.
```bash
$ dcos {{ model.packageName }} debug pod pause <pod-name> -t <task-name>
```



## Metrics

Both service task and service scheduler metrics are pushed into the [DC/OS metrics system](/latest/metrics/).

### Service Scheduler Metrics
The service scheduler records a number of metrics that can be used to diagnose issues with the scheduler and monitor the performance of the scheduler. The metrics can be consumed via DC/OS metrics, or pulled directly from the service scheduler.

A JSON representation of the metrics is available at the `/v1/metrics` endpoint of the service scheduler.

####### JSON
```json
{
	"version": "3.1.3",
	"gauges": {},
	"counters": {
		"declines.long": {
			"count": 15
		},
		"offers.processed": {
			"count": 18
		},
		"offers.received": {
			"count": 18
		},
		"operation.create": {
			"count": 5
		},
		"operation.launch_group": {
			"count": 3
		},
		"operation.reserve": {
			"count": 20
		},
		"revives": {
			"count": 3
		},
		"task_status.task_running": {
			"count": 6
		}
	},
	"histograms": {},
	"meters": {},
	"timers": {
		"offers.process": {
			"count": 10,
			"max": 0.684745927,
			"mean": 0.15145255818999337,
			"min": 5.367950000000001E-4,
			"p50": 0.0035879090000000002,
			"p75": 0.40317217800000005,
			"p95": 0.684745927,
			"p98": 0.684745927,
			"p99": 0.684745927,
			"p999": 0.684745927,
			"stddev": 0.24017017290826104,
			"m15_rate": 0.5944843686231079,
			"m1_rate": 0.5250565015924039,
			"m5_rate": 0.583689104996544,
			"mean_rate": 0.3809369986002824,
			"duration_units": "seconds",
			"rate_units": "calls/second"
		}
	}
}
```

A Prometheus representation of the metrics is available at the `/v1/metrics/prometheus` endpoint of the service scheduler.
####### Prometheus
```
# HELP declines_long Generated from Dropwizard metric import (metric=declines.long, type=com.codahale.metrics.Counter)
# TYPE declines_long gauge
declines_long 20.0
# HELP offers_processed Generated from Dropwizard metric import (metric=offers.processed, type=com.codahale.metrics.Counter)
# TYPE offers_processed gauge
offers_processed 24.0
# HELP offers_received Generated from Dropwizard metric import (metric=offers.received, type=com.codahale.metrics.Counter)
# TYPE offers_received gauge
offers_received 24.0
# HELP operation_create Generated from Dropwizard metric import (metric=operation.create, type=com.codahale.metrics.Counter)
# TYPE operation_create gauge
operation_create 5.0
# HELP operation_launch_group Generated from Dropwizard metric import (metric=operation.launch_group, type=com.codahale.metrics.Counter)
# TYPE operation_launch_group gauge
operation_launch_group 4.0
# HELP operation_reserve Generated from Dropwizard metric import (metric=operation.reserve, type=com.codahale.metrics.Counter)
# TYPE operation_reserve gauge
operation_reserve 20.0
# HELP revives Generated from Dropwizard metric import (metric=revives, type=com.codahale.metrics.Counter)
# TYPE revives gauge
revives 4.0
# HELP task_status_task_finished Generated from Dropwizard metric import (metric=task_status.task_finished, type=com.codahale.metrics.Counter)
# TYPE task_status_task_finished gauge
task_status_task_finished 1.0
# HELP task_status_task_running Generated from Dropwizard metric import (metric=task_status.task_running, type=com.codahale.metrics.Counter)
# TYPE task_status_task_running gauge
task_status_task_running 8.0
# HELP offers_process Generated from Dropwizard metric import (metric=offers.process, type=com.codahale.metrics.Timer)
# TYPE offers_process summary
offers_process{quantile="0.5",} 2.0609500000000002E-4
offers_process{quantile="0.75",} 2.2853200000000001E-4
offers_process{quantile="0.95",} 0.005792643
offers_process{quantile="0.98",} 0.005792643
offers_process{quantile="0.99",} 0.111950848
offers_process{quantile="0.999",} 0.396119612
offers_process_count 244.0
```
